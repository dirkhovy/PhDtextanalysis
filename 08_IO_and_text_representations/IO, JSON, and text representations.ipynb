{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text I-O and JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *'Anything goes in/ Anything goes out/ Fish, bananas, old pyjamas/ Mutton, beef and Trout!'*\n",
    "*â€“ User manual for first PC*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Input/Output (IO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading** from input and **writing** to output allows us to retrieve or store information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Input\n",
    "\n",
    "### Reading from files\n",
    "\n",
    "Python makes it very easy to read from files. Calling `open()` on a file name will give you an iterator over the lines in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 0\n",
    "for line in open('../data/tweets.txt', 'r'): # this is BAD coding, don't repeat\n",
    "    num_lines += 1\n",
    "print(\"The file has {} lines\".format(num_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this does not close the file, and it is better to make sure we do. We use the `with` construct. The outermost level simply open the file and give it a name, and closes it when done. The inner loop goes over the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 0\n",
    "with open('../data/tweets.txt', encoding='utf-8') as input_file:\n",
    "    for line in input_file:\n",
    "        num_lines += 1\n",
    "print(\"The file has {} lines\".format(num_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activity\n",
    "* count the number of words in `tweets.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading user input\n",
    "Sometimes, we want to give our user the chance to input something (hint: *very* useful to label data). We can simply do this via `input()`. If we give it an argument, we can write a message to the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_says = input('what ')\n",
    "print(user_says, type(user_says))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NB: the return value of `input()` is always a `str`. If you want something else, you neeed to either cast it, or use `eval()`, which interprets the input as Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_says = input('Try typing an int or list: ')\n",
    "print(type(eval(user_says)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To prevent empty input errors, or to check the user makes a valid choice, use a `while` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_be_int = None\n",
    "while not must_be_int or must_be_int not in {'1', '2'}:\n",
    "    must_be_int = input('Type 1 or 2: ')\n",
    "must_be_int = eval(must_be_int)\n",
    "print(must_be_int, type(must_be_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Output\n",
    "\n",
    "### User Output\n",
    "\n",
    "Output to the user is our good old friend `print()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### File Output\n",
    "\n",
    "File output allows us to use the same Python objects in different programs/sessions/computers. It works almost like file input, with three differences:\n",
    "1. we need to specify write mode by giving `open()` the string argument `'w'`\n",
    "2. we use the `write()` command to write to the file\n",
    "3. we need to end every input line with a newline break `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/silly_test_file.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('This is the first line\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activity\n",
    "\n",
    "* Open the file `silly_test_file.txt` and print all the lines in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# JSON\n",
    "\n",
    "JSON is a file format that allows us to read and write Python objects (rather than strings) from files. This is a great way to save your progress or to store a model.\n",
    "\n",
    "However, note that dictionary keys become strings, and that it cannot store \"special\" data types (`defaultdict`, `DataFrame`, etc.).\n",
    "\n",
    "We need to import the `json` library first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# JSON output\n",
    "\n",
    "In order to save a Python object to a file, we only need the function `dump()` from `json`. It takes two arguments\n",
    "1. the Python object to write to file\n",
    "2. a **file handle**, i.e., an `open(<FILENAME>, 'w')` command\n",
    "\n",
    "You can call JSON files whatever you want, but it is common to use the ending `.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Activity\n",
    "\n",
    "* create a dictionary `line_length`\n",
    "* open the file `tweets.txt`\n",
    "* use `line_length` to map each line in the file from its line number to its length in characters\n",
    "* save `line_length` to a file \"`lineinfo.json`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# JSON input\n",
    "\n",
    "To retrive a Python object from a file, we use the function `load()` from `json`. It only takes a **file handle**, i.e., an `open(<FILENAME>)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lineinfo.json') as json_in:\n",
    "    info_from_file = json.load(json_in)\n",
    "print(list(info_from_file.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Representations\n",
    "\n",
    "## *\"I know words. I have the best words!\"*\n",
    "    - Noam Chomsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete Sparse Representations (Bag of words)\n",
    "\n",
    "The easiest way is to represent features is as a counts of all words in the text. It takes two steps:\n",
    "1. collect the counts for each word\n",
    "2. transform the individual counts into one big matrix\n",
    "\n",
    "The result is a matrix $X$ with one row for each instance, and one column for each word in the vocabulary.\n",
    "\n",
    "![Bag of words procedure](bow.png)\n",
    "\n",
    "We can use the `TfidfVectorizer` object to get the weighted frequency of each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/reviews.full.tsv', sep='\\t', nrows=100000)\n",
    "documents = df.text.tolist()\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "small_vectorizer = CountVectorizer()\n",
    "\n",
    "sentences_2 = documents[:1]\n",
    "\n",
    "X1 = small_vectorizer.fit_transform(sentences_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to deal with linear algebra\n",
    "\n",
    "num_docs = 1\n",
    "\n",
    "# collect all word types (= vocabulary)\n",
    "vocabulary = set()\n",
    "for document in documents[:num_docs]:\n",
    "    tokens = document.lower().split()\n",
    "    vocabulary = vocabulary.union(set(tokens))\n",
    "vocabulary = sorted(vocabulary)\n",
    "\n",
    "# create a data matrix with #docs-by-#features dimensions\n",
    "X = np.zeros((num_docs, len(vocabulary)))\n",
    "\n",
    "# fill that matrix with sweet counts\n",
    "for d, document in enumerate(documents[:num_docs]):\n",
    "    tokens = document.lower().split()\n",
    "    for i, feature in enumerate(vocabulary):\n",
    "        X[d, i] = tokens.count(feature)\n",
    "\n",
    "# show the result as a DataFrame\n",
    "pd.DataFrame(data=X, columns=vocabulary, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_ = {word: position for position, word in enumerate(vocabulary)}\n",
    "vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The result is a *sparse count matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexed representation\n",
    "import numpy as np\n",
    "print(X1)\n",
    "\n",
    "# dense representation\n",
    "print(X1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can access the mapping from vector position to feature names via `get_feature_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inverse (the mapping from feature names to vector positions) is encoded as a list in `vocabulary_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Terminology \n",
    "\n",
    "![](matrix.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's redo this for the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', \n",
    "                             ngram_range=(1, 2), \n",
    "                             min_df=0.001, \n",
    "                             max_df=0.75, \n",
    "                             stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(documents[:10000])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `transform()` on a new document will aply the vocabulary we collected previously to this new data point. Any words we have not seen before are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform([documents[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Use vector operations to find out \n",
    "- what the 5 most frequent words are in `X`\n",
    "- in how many different documents the word `delivery` occurs\n",
    "- what percentage of the overall corpus that number corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Character $n$-grams\n",
    "\n",
    "We can also use characters to analyze text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = CountVectorizer(analyzer='char', \n",
    "                                  ngram_range=(2, 6), \n",
    "                                  min_df=0.001, \n",
    "                                  max_df=0.75)\n",
    "\n",
    "C = char_vectorizer.fit_transform(documents[:10])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Extract **only** the bigrams (no unigrams) from Moby Dick and find the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Let's extract the most important words from Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [line.strip() for line in open('../data/moby_dick.txt', encoding='utf8')]\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', min_df=0.001, max_df=0.75, stop_words='english', sublinear_tf=True)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the same information as raw counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.001, max_df=0.75, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI\n",
    "Extracting PMI from text is relatively straightforward, and `nltk` offer some functions to do so flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "words = [word.lower() for document in documents for word in document.split() \n",
    "         if len(word) > 2 \n",
    "         and word not in stopwords_]\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "bgm = BigramAssocMeasures()\n",
    "score = bgm.mi_like\n",
    "collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(collocations).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Extract the top 10 collocations for the Twitter data. You need to preprocess the data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
