{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *\"Words. I know words. I have the best words!\"*\n",
    "*- Noam Chomsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "<video controls src=\"cartoon.m4v\" type=\"video/mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment Analysis: Implementation\n",
    "\n",
    "<video controls src=\"machine_learning.m4v\" type=\"video/mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "In order to train a machine learning model to classify text, we need:\n",
    "1. a way to preprocess text\n",
    "2. a label for each text, represented as number\n",
    "3. a way to represent each text as vector input\n",
    "4. a model to learn  a function $f(input) = label$\n",
    "5. a way to evaluate how well the model works\n",
    "6. a way to predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example, we will use reviews data and try to classify the rating into $positive$ or $negative$, only based on the text they use.\n",
    "\n",
    "The same method can be used for any other data, including more labels and other dependent variables (e.g., age or gender of the text author, social constructs expressed in the text, etc...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Getting data\n",
    "\n",
    "We use `pandas` to read in our data from a CSV file, but you can use almost any format (remember `read_excel()`, `read_sql()`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/sa_train.csv')\n",
    "print(len(data), data['output'].unique())\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''reduce text to lower-case lexicon entry'''\n",
    "    lemmas = [token.lemma_ for token in nlp(text) \n",
    "              if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "clean_text('This is a test sentence. And here comes another one... Go me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the input data. This can take a while, so it's good to save it... I have done that here already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['clean_text'] = data['input'].apply(clean_text)\n",
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Labels\n",
    "\n",
    "Here, we assume that we already have the labels. (In your task, you will have to label them yourself! Hint: use `input()` or a spreadsheet).\n",
    "\n",
    "However, in order for the machine learning model to work with the labels, we need to translate them into a vector of numbers. We can use `sklearn.LabelEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# transform labels into numbers\n",
    "labels2numbers = LabelEncoder()\n",
    "\n",
    "y = labels2numbers.fit_transform(data['output'])\n",
    "print(data['output'][:10], y[:10], len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the original names back, use `inverse_transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2numbers.inverse_transform([1,1,1,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Representing text\n",
    "\n",
    "First, we need to transform the texts into a matrix, where each row represents one text instance. The columns are the **features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0.001, max_df=0.75, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(data.clean_text)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trasnlate back and forth between columns and words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[4191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# [vectorizer.get_feature_names()[f] for f in (X.sum(axis=1)).A1.argsort()[:10]]\n",
    "vectorizer.get_feature_names()[1244]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often that word is in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.input.str.contains('bad')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Learning a classification model\n",
    "\n",
    "A classification model is simply a function that takes a text representation as input, and returns an output label.\n",
    "\n",
    "Inside that function is normally a set of weights. By multiplying the weight vector with the input vector, we get the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.1: Fitting a model\n",
    "\n",
    "Fitting a model is the process of finding the right weights to map the training inputs to the training outputs. Fitting to data in `sklearn` is easy: we use the `fit()` function, giving it the input matrix and output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "%time classifier.fit(X, y)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting fitted model has coefficients (betas) for each word/feature in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now examine the weights/coefficients/betas for the individual words (note that each word has an ID):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = vectorizer.vocabulary_['bad'] # column position for the word\n",
    "print(vectorizer.get_feature_names()[k], classifier.coef_[0, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: in a two-class problem, our coefficents are in a vector: positive values indicate the positive class, negative values the other class.\n",
    "In a multi-class problem, we have one **row** of coefficients for each class: positive values indicate that this feature contributes to the class, negative values indicate that it contributes to other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Evaluating models\n",
    "\n",
    "Having a model is great, but how well does it do? Can it classify what it has seen? We need a way to estimate how well the model will work on new data.\n",
    "\n",
    "We need a metric to measure performance and a way to simulate new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.1: Metrics\n",
    "\n",
    "We use three measure:\n",
    "1. precision\n",
    "2. recall\n",
    "3. F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures how many of our model's predictions were correct. We divide the number of true positives by the number of all positives\n",
    "\n",
    "$$\n",
    "p = \\frac{tp}{tp+fp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "Recall measures how many of the correct answers in the data our model managed to find. We divide the number of true positives by the number of true positives (the instances our model got) and false negatives (the instances our model *should* have gotten)\n",
    "\n",
    "$$\n",
    "r = \\frac{tp}{tp+fn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1\n",
    "\n",
    "A model that classified everything as, say, \"positive\" would get a perfect recall (it does, after all, find all positive examples). However, such a model would obviously be useless, since its precision is bad.\n",
    "\n",
    "We want to balance the two against each other. F1 does exactly that, by taking the harmonic mean.\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{p\\cdot r}{p+r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Luckily, all of these metrics are implemented in `sklearn`. All we have to provide are the predictions of our model, and the actual correct answers (called the *gold standard*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2: Cross-validation\n",
    "\n",
    "How do we measure performance on new data, if we don't know what the correct outputs for those new data points are?\n",
    "\n",
    "In **$k$-fold cross-validation**, we simulate new data, by fitting our model on parts of the data, and evaluating on other. We can thereby measure the performance on the held-out part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we have now reduced the amount of data we used to fit the data. In order to address this, we simply repeat the process $k$ times.\n",
    "We separate the data into $k$ parts, fit the model on $k-1$ parts, and evaluate on the $k$th part. In the end, we have performance scores from $k$ models. The average of them tells us how well the model would work on new data.\n",
    "\n",
    "![3-fold cross-validation](3foldCV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for k in [2,3,5,10]:\n",
    "    cv = cross_val_score(LogisticRegression(), X, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\")\n",
    "    fold_size = X.shape[0]/k\n",
    "    \n",
    "    print(\"F1 with {} folds for bag-of-words is {}\".format(k, cv.mean()))\n",
    "    print(\"Training on {} instances/fold, testing on {}\".format(fold_size*(k-1), fold_size))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "So, is that performance good? Let's compare to a **baseline**, i.e., a null-hypothesis. The simplest one is that all instances belong to the most fereuqnt class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "print(cross_val_score(most_frequent, X, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "See whether you can apply the previous steps to a new data sets, a description of wines. Choose any of the descriptor columns as target variable. The text is already preprocessed, to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('../data/wine.csv', encoding='utf8')\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 6 Classifying new data\n",
    "\n",
    "Classifying new (**held-out**) data is called **prediction**. We reuse the weights we have learned before on a new data matrix to predict the new outcomes.\n",
    "Important: the new data needs to have the same number of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in new data set\n",
    "new_data = pd.read_csv('../data/sa_test.csv')\n",
    "print(len(new_data))\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to clean it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time new_data['clean_text'] = new_data.input.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how well we do on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text into word counts\n",
    "# IMPORTANT: use same vectorizer we fit on training data to create vectors!\n",
    "new_X = vectorizer.transform(new_data.clean_text)\n",
    "\n",
    "# use the old classifier to predict and evaluate\n",
    "new_predictions = classifier.predict(new_X)\n",
    "print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can also predict the probabilities of belonging to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_probabilities = classifier.predict_proba(new_X)\n",
    "print(new_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance (=row), we get a probability distribution over the classes (=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.1 Regularization\n",
    "\n",
    "Typically, performance is lower on unseen data, because our model **overfit** the training data: it expects the new data to look *exactly* the same as the training data. That is almost never true.\n",
    "\n",
    "In order to prevent the model from overfitting, we need to **regularize** it. Essentially, we make it harder to learn the training data.\n",
    "\n",
    "A simple example of regularization is to \"corrupt\" the training data by adding a little bit of noise to each training instance. Since the noise is irregular, it becomes harder for the model to learn any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import random\n",
    "\n",
    "num_instances, num_features = X.shape\n",
    "\n",
    "for i in range(5):\n",
    "    X_regularized = X + random(num_instances, num_features, density=0.01)\n",
    "\n",
    "    print(cross_val_score(LogisticRegression(), X_regularized, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you run the previous cell several times, you see different results (it gets even more varied if you change `density`). This variation arises because we add **random** noise. Not good...\n",
    "\n",
    "Instead, it makes sense to force the model to spread the weights more evenly over all features, rather than bet on a few feature, which mighht not be present in future data.\n",
    "\n",
    "We can do this by training the model with the `C` parameter. The default is `1`. Lower values mean stricter regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "best_c = None\n",
    "best_f1_score = 0.0\n",
    "for c in [50, 20, 10, 1.0, 0.5, 0.1, 0.05, 0.01]:\n",
    "    clf = LogisticRegression(C=c)\n",
    "    cv_reg = cross_val_score(clf, X, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\").mean()\n",
    "\n",
    "    print(\"5-CV on train at C={}: {}\".format(c, cv_reg.mean()))\n",
    "    print()\n",
    "\n",
    "    if cv_reg > best_f1_score:\n",
    "        best_f1_score = cv_reg\n",
    "        best_c = c\n",
    "        \n",
    "print(\"best C parameter: {}\".format(best_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better features = better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now have **a lot** of features! More than we have actual examples...\n",
    "\n",
    "Not all of them will be helpful, though. Let's select the top 1500 based on how well they predict they outcome of the training data.\n",
    "\n",
    "We use two libraries from `sklearn`, `SelectKBest` (the selection algorithm) and `chi2` (the selection criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=1500).fit(X, y)\n",
    "X_sel = selector.transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how well this new representation performs, by looking at the 5-fold cross-validation. We keep the best regularization value from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=best_c)\n",
    "\n",
    "cv_reg = cross_val_score(clf, X_sel, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\")\n",
    "print(\"5-CV on train: {}\".format(cv_reg.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not too bad! We have handily beaten our previous best! Let's fit a classifier on the whole data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_sel, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's apply it to the held-out data set. \n",
    "We need to \n",
    "* vectorize the data with our vectorizer from before (otherwise, we get different features)\n",
    "* select the top features (using our previously fitted selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for new data\n",
    "new_X_sel = selector.transform(new_X)\n",
    "print(new_X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can use our new classifier to predict the new data labels, and compare them to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_regularized = clf.predict(new_X_sel)\n",
    "prediction_df = pd.DataFrame(data={'input': new_data['input'], 'prediction': labels2numbers.inverse_transform(new_predictions_regularized), 'truth':new_data['output']})\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['input'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting insights\n",
    "\n",
    "In order to explore which features are most indicative, we need some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names() # get the names of the features\n",
    "top_scores = selector.scores_.argsort()[-1500:] # get the indices of the selection\n",
    "best_indicator_terms = [features[i] for i in sorted(top_scores)] # sort feature names\n",
    "\n",
    "top_indicator_scores = pd.DataFrame(data={'feature': best_indicator_terms, 'coefficient': clf.coef_[0]})\n",
    "top_indicator_scores.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Checklist: how to classify my data\n",
    "\n",
    "1. label at ***least 2000*** tweets in your data set as `positive`, `negative`, or `neutral`\n",
    "2. preprocess the text of *all* tweets in your data (labeled and unlabeled)\n",
    "3. read in the labeled tweets and their labels\n",
    "4. transform the labels into numbers\n",
    "5. use `TfidfVectorizer` to extract the features and transform them into feature vectors\n",
    "6. select the top $N$ features (where $N$ is smaller than the number of labeled tweets)\n",
    "7. create a classifier\n",
    "8. use 5-fold CV to find the best regularization parameter, top $N$ feature selection, and maybe feature generation and preprocessing steps\n",
    "\n",
    "Once you are satisfied with the results:\n",
    "9. read in the rest of the (unlabeled) tweets\n",
    "10. use the `TfidfVectorizer` from 5. to transform the new data into vectors\n",
    "11. use the `SelectKBest` selector from 6. to get the top $N$ features\n",
    "12. use the classifier from 7. to predict the labels for the new data\n",
    "13. save the predicted labels or probabilities to your database or an Excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
