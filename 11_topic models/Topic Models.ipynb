{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104637"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/wine.csv', encoding='utf8')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "      <th>description_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>tremendous varietal wine hail be age year oak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>honor memory wine once make his mother tremend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>spend month new french oak incorporate fruit v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This re-named vineyard was formerly bottled as...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>re name vineyard be formerly bottle will find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The producer sources from two blocks of the vi...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>producer source block vineyard wine high eleva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description country    province  \\\n",
       "0  This tremendous 100% varietal wine hails from ...      US  California   \n",
       "1  Mac Watson honors the memory of a wine once ma...      US  California   \n",
       "2  This spent 20 months in 30% new French oak, an...      US      Oregon   \n",
       "3  This re-named vineyard was formerly bottled as...      US      Oregon   \n",
       "4  The producer sources from two blocks of the vi...      US  California   \n",
       "\n",
       "              variety                                description_cleaned  \n",
       "0  Cabernet Sauvignon  tremendous varietal wine hail be age year oak ...  \n",
       "1     Sauvignon Blanc  honor memory wine once make his mother tremend...  \n",
       "2          Pinot Noir  spend month new french oak incorporate fruit v...  \n",
       "3          Pinot Noir  re name vineyard be formerly bottle will find ...  \n",
       "4          Pinot Noir  producer source block vineyard wine high eleva...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is how the data was cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# def clean(text):\n",
    "#     return ' '.join([token.lemma_ \n",
    "#             for token in nlp(text) \n",
    "#             if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'X'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104637"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty entries\n",
    "df = df[df['description_cleaned'].notnull()]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import time # to know how long training took\n",
    "import multiprocessing # to speed things up by parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=50000\n",
    "\n",
    "# get dictionary\n",
    "df['description_cleaned'] = df.description_cleaned.apply(str)\n",
    "# run on 50000 instances\n",
    "instances = df.description_cleaned.apply(str.split)[:limit]\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.3)\n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances]\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instances[0])\n",
    "print(ldacorpus[0]) \n",
    "print(model_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "\n",
    "dev_size = 10000\n",
    "eval_size = 5000\n",
    "\n",
    "for num_topics in range(5, 16):\n",
    "    model = LdaMulticore(corpus=model_corpus[:dev_size], \n",
    "                         id2word=dictionary, \n",
    "                         num_topics=num_topics)\n",
    "\n",
    "    coherencemodel_umass = CoherenceModel(model=model, \n",
    "                                          texts=instances[dev_size:dev_size+eval_size], \n",
    "                                          dictionary=dictionary, \n",
    "                                          coherence='u_mass')\n",
    "\n",
    "    coherencemodel_cv = CoherenceModel(model=model, \n",
    "                                       texts=instances[dev_size:dev_size+eval_size], \n",
    "                                       dictionary=dictionary, \n",
    "                                       coherence='c_v')\n",
    "\n",
    "    umass_score = coherencemodel_umass.get_coherence()\n",
    "    cv_score = coherencemodel_cv.get_coherence()\n",
    "    \n",
    "    print(num_topics, umass_score, cv_score)\n",
    "    coherence_values.append((num_topics, umass_score, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context('poster') # use large font\n",
    "\n",
    "\n",
    "scores = pd.DataFrame(coherence_values, columns=['num_topics', 'UMass', 'CV'])\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "scores.plot.line(x='num_topics', y='UMass', ax=ax[0], xticks=range(5,21));\n",
    "scores.plot.line(x='num_topics', y='CV', ax=ax[1], xticks=range(5,21));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data into topic distros\n",
    "topic_corpus = model[model_corpus]\n",
    "\n",
    "topic_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aggregating topics by a dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_category = 'country'\n",
    "# get a list of all the topic scores for each document\n",
    "scores = [[t[1] for t in topic_corpus[entry]] for entry in range(limit)]\n",
    "# turn that into a data frame with N rows and K columns, each with the score of the corresponding topic\n",
    "topic_distros = pd.DataFrame(data=scores, columns=descriptors)\n",
    "# add the review category of each document (so we can aggregate)\n",
    "topic_distros['category'] = df[target_category][:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # make graphs\n",
    "import seaborn # make prettier graphs\n",
    "\n",
    "seaborn.set_context('poster') # use large font\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10)) # set graph size\n",
    "# aggregate topics by categories\n",
    "aggregate_by_category = topic_distros[topic_distros.category.isin('Germany US Italy France Spain'.split())]\n",
    "aggregate_by_category = aggregate_by_category.groupby(aggregate_by_category.category).mean()\n",
    "# plot the graph\n",
    "aggregate_by_category[descriptors].plot.bar(ax=ax);\n",
    "# move the legend out\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Author Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import datapath, temporary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "author2doc = defaultdict(list)\n",
    "\n",
    "for i, country in enumerate(df.country[:limit]):\n",
    "    author2doc[country].append(i)\n",
    "    \n",
    "len(author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "author_model_list = []\n",
    "\n",
    "dev_size = 10000\n",
    "eval_size = 10000\n",
    "\n",
    "dev_author2doc = {key: [idx for idx in value if idx < dev_size] for key, value in author2doc.items()}\n",
    "\n",
    "for num_topics in range(5, 16):\n",
    "    author_model = AuthorTopicModel(corpus=list(model_corpus[:dev_size]), \n",
    "                                    author2doc=dev_author2doc, \n",
    "                                    id2word=dictionary, \n",
    "                                    num_topics=num_topics)\n",
    "#     author_model_list.append(author_model)\n",
    "    \n",
    "    coherencemodel_umass = CoherenceModel(model=author_model, \n",
    "                                          texts=instances[dev_size:dev_size+eval_size], \n",
    "                                          dictionary=dictionary, \n",
    "                                          coherence='u_mass')\n",
    "\n",
    "    coherencemodel_cv = CoherenceModel(model=author_model, \n",
    "                                       texts=instances[dev_size:dev_size+eval_size], \n",
    "                                       dictionary=dictionary, \n",
    "                                       coherence='c_v')\n",
    "\n",
    "    umass_score = coherencemodel_umass.get_coherence()\n",
    "    cv_score = coherencemodel_cv.get_coherence()\n",
    "    \n",
    "    print(num_topics, umass_score, cv_score)\n",
    "    coherence_values.append((num_topics, umass_score, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scores = pd.DataFrame(coherence_values, columns=['num_topics', 'UMass', 'CV'])\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "scores.plot.line(x='num_topics', y='UMass', ax=ax[0], xticks=range(5,16));\n",
    "scores.plot.line(x='num_topics', y='CV', ax=ax[1], xticks=range(5,16));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_author = 8\n",
    "\n",
    "\n",
    "author_model = AuthorTopicModel(corpus=list(model_corpus), \n",
    "                                author2doc=author2doc, \n",
    "                                id2word=dictionary, \n",
    "                                num_topics=n_topics_author,\n",
    "                                passes=num_passes,\n",
    "                                chunksize=chunk_size,\n",
    "                                alpha=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "author_model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                author_model.print_topics(num_topics=n_topics_author, num_words=5)]\n",
    "\n",
    "author_descriptors = []\n",
    "for i, m in sorted(author_model_topics):\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    author_descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_vecs = {author: {author_descriptors[t]: 0.0\n",
    "                         for t in range(author_model.num_topics)}\n",
    "              for author in author_model.id2author.values()\n",
    "              }\n",
    "for author in author_model.id2author.values():\n",
    "    for (t, v) in author_model.get_author_topics(author):\n",
    "        author_vecs[author][author_descriptors[t]] = v\n",
    "\n",
    "for country in 'Germany US Italy France Spain'.split():\n",
    "    print(country, author_vecs[country])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = pd.DataFrame.from_dict(author_vecs)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "author_df['Germany US Italy France Spain'.split()].T.plot.bar(ax=ax)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vectorize input\n",
    "vectorizer = CountVectorizer(analyzer='word', \n",
    "                             ngram_range=(1,2), \n",
    "                             min_df=100, \n",
    "                             max_df=0.3, \n",
    "                             stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(df.description_cleaned[:limit].tolist())\n",
    "\n",
    "# store lookup structures for convenience\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic seeds based on intuition\n",
    "indicators = {\n",
    "    'BRIGHT': [\"fruity\", \"crisp\", \"bright\"],\n",
    "    'SPICE': [\"licorice\", \"pepper\", 'spice'], \n",
    "    'GREEN FRUIT': [\"apple\", \"lemon\", \"citrus\", \"peach\", \"pear\"],\n",
    "    'DARK': [\"vanilla\", \"smoke\", \"leather\"],\n",
    "    'RED FRUIT': [\"strawberry\", \"raspberry\", 'cherries'],\n",
    "    'FOOD': [\"pair\", \"food\", \"steak\"],\n",
    "    'FULL': [\"bodied\", \"smoke\", \"medium\", \"vanilla\"],\n",
    "    'AGEING': [\"age\", \"year\", \"structure\"]\n",
    "}\n",
    "\n",
    "topic_names, seed_topic_list = zip(*indicators.items())\n",
    "# filter out all words not actually in vocab\n",
    "seed_topic_list = [[w for w in words if w in set(vocab)] for words in seed_topic_list]\n",
    "    \n",
    "# create a mapping {word_id: k}, which we need for the model\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "        \n",
    "print(seed_topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = guidedlda.GuidedLDA(n_topics=len(seed_topic_list), \n",
    "                            n_iter=1000, \n",
    "                            random_state=7, \n",
    "                            refresh=50, \n",
    "                            alpha=0.5, \n",
    "                            eta=0.000001)\n",
    "\n",
    "# fit the model with seeds\n",
    "doc_topic = model.fit_transform(X, seed_topics=seed_topics, seed_confidence=10)\n",
    "\n",
    "# retrieve the word descriptors\n",
    "n_top_words = 5\n",
    "topic_word = model.topic_word_\n",
    "descriptors_guided = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: \"{}\"'.format(topic_names[i], '\" \"'.join(topic_words)))\n",
    "    descriptors_guided.append(' '.join(topic_words[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the topic scores for each document\n",
    "# scores = [[t[1] for t in topic_corpus[entry]] for entry in range(limit)]\n",
    "# turn that into a data frame with N rows and K columns, each with the score of the corresponding topic\n",
    "topic_distros_guided = pd.DataFrame(data=model.doc_topic_, columns=descriptors_guided)\n",
    "# add the review category of each document (so we can aggregate)\n",
    "topic_distros_guided['category'] = df[target_category][:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10)) # set graph size\n",
    "# aggregate topics by review categories\n",
    "aggregate_by_category = topic_distros_guided[topic_distros_guided.category.isin('Germany US Italy France Spain'.split())]\n",
    "aggregate_by_category = aggregate_by_category.groupby(aggregate_by_category.category).mean()\n",
    "\n",
    "# plot the graph\n",
    "aggregate_by_category[descriptors_guided].plot.bar(ax=ax);\n",
    "# move the legend out\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
