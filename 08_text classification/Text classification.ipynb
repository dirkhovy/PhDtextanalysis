{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *\"Words. I know words. I have the best words!\"*\n",
    "*- Noam Chomsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "<video controls src=\"cartoon.m4v\" type=\"video/mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment Analysis: Implementation\n",
    "\n",
    "<video controls src=\"machine_learning.m4v\" type=\"video/mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "In order to train a machine learning model to classify text, we need:\n",
    "1. a way to preprocess text\n",
    "2. a label for each text, represented as number\n",
    "3. a way to represent each text as vector input\n",
    "4. a model to learn  a function $f(input) = label$\n",
    "5. a way to evaluate how well the model works\n",
    "6. a way to predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example, we will use reviews data and try to classify the rating into $positive$ or $negative$, only based on the text they use.\n",
    "\n",
    "The same method can be used for any other data, including more labels and other dependent variables (e.g., age or gender of the text author, social constructs expressed in the text, etc...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Getting data\n",
    "\n",
    "We use `pandas` to read in our data from a CSV file, but you can use almost any format (remember `read_excel()`, `read_sql()`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 ['neg' 'pos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>shakespeare in love is quite possibly the most...</td>\n",
       "      <td>neg</td>\n",
       "      <td>shakespeare love be quite possibly most enjoya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wizards is an animated feature that begins wit...</td>\n",
       "      <td>neg</td>\n",
       "      <td>wizard be animate feature that begin narration...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              input output  \\\n",
       "0           0  shakespeare in love is quite possibly the most...    neg   \n",
       "1           1  wizards is an animated feature that begins wit...    neg   \n",
       "\n",
       "                                          clean_text  \n",
       "0  shakespeare love be quite possibly most enjoya...  \n",
       "1  wizard be animate feature that begin narration...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('sa_train.csv')\n",
    "print(len(data), data['output'].unique())\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be test sentence here come one go'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    '''reduce text to lower-case lexicon entry'''\n",
    "    lemmas = [token.lemma_ for token in nlp(text) \n",
    "              if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "clean_text('This is a test sentence. And here comes another one... Go me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the input data. This can take a while, so it's good to save it... I have done that here already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       shakespeare love be quite possibly most enjoya...\n",
       "1       wizard be animate feature that begin narration...\n",
       "2       gun wield arnold schwarzenegger have change he...\n",
       "3       keep jane austen sense sensibility pride preju...\n",
       "4       hollywood be pimp fat cigar smoke chump wear f...\n",
       "5       look new version psycho come world didn t end ...\n",
       "6       film adapt comic book have have plenty success...\n",
       "7       capsule verma family be have wedding all relat...\n",
       "8       watch battlefield earth be wallow misery be mo...\n",
       "9       tommy lee jone chase innocent victim america w...\n",
       "10      michael robbin hardball be quite cinematic ach...\n",
       "11      s almost amusing watch year old christina ricc...\n",
       "12      accord hollywood movie make last few decade li...\n",
       "13      young einstein be embarrassingly lame didn t s...\n",
       "14      kirk dougla be rare american actor who can say...\n",
       "15      man be not man tael gold star sammo hang sylvi...\n",
       "16      director nightmare christma say preview which ...\n",
       "17      phil curtolo mel gibson braveheart give grip p...\n",
       "18      chris farley have strap fake mutton chop sideb...\n",
       "19      house be bear bad go haunting s tag line which...\n",
       "20      day lack originality hollywood reflect deluge ...\n",
       "21      movie teenager teenage culture rarely prove be...\n",
       "22      let s face waterworld float by summer movie se...\n",
       "23      love kill start aimlessly get progressively le...\n",
       "24      astronaut wife charlize theron play young woma...\n",
       "25      thing do first time film director tom hank be ...\n",
       "26      film open expectant unwed mother sally play dr...\n",
       "27      there s more quilt fabric thread patchwork des...\n",
       "28      may seem weird begin film glam rock sequence t...\n",
       "29      capsule be style heist film set present robert...\n",
       "                              ...                        \n",
       "1770    love movie ve just see umptenth time unfortuna...\n",
       "1771    maybe mission should have be scrub mission mar...\n",
       "1772    there be scene patch adam which patch be cente...\n",
       "1773    -PRON- giant be movie price be worth cost admi...\n",
       "1774    last night could have aka tag end that say s e...\n",
       "1775    be gentle urge natasha henstridge matthew perr...\n",
       "1776    _ soldier _ be hand down bad movie person coul...\n",
       "1777    enemy gate jude law be gifted russian sniper m...\n",
       "1778    first reel girl town just can t get lili taylo...\n",
       "1779    star armand assante mike hammer barbara carrer...\n",
       "1780    -PRON- exceedingly well do visual effect origi...\n",
       "1781    vampire s be rude chauvinistic movie where wom...\n",
       "1782    have confession even be movie junkie see eye b...\n",
       "1783    bob happy bastard s quickie review must admit ...\n",
       "1784    scene end s dead poet society when robin willi...\n",
       "1785    luckily people get starship trooper people kno...\n",
       "1786    follow review contain spoiler s just way be ra...\n",
       "1787    screen adaptation john irv novel have be disap...\n",
       "1788    retell classic story joan arc have be popular ...\n",
       "1789    ever wargame first real computer hacking movie...\n",
       "1790    wyatt earp have lot tell little say story lege...\n",
       "1791    synopsis captain picard crew starship enterpri...\n",
       "1792    be bad movie ve view so far avenger silly man ...\n",
       "1793    almost full decade steven spielberg s save pri...\n",
       "1794    lady gentleman s independence day be here s ti...\n",
       "1795    terrence malick make excellent minute film ada...\n",
       "1796    should know summer have be less memorable tota...\n",
       "1797    advertisement didn t even try conceal fact mov...\n",
       "1798    movie divorce custody seem about as timely mov...\n",
       "1799    plot down girl move top model fall love goofy ...\n",
       "Name: clean_text, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['clean_text'] = data['input'].apply(clean_text)\n",
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Labels\n",
    "\n",
    "Here, we assume that we already have the labels. (In your task, you will have to label them yourself! Hint: use `input()` or a spreadsheet).\n",
    "\n",
    "However, in order for the machine learning model to work with the labels, we need to translate them into a vector of numbers. We can use `sklearn.LabelEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    neg\n",
      "1    neg\n",
      "2    neg\n",
      "3    pos\n",
      "4    pos\n",
      "5    neg\n",
      "6    pos\n",
      "7    pos\n",
      "8    neg\n",
      "9    neg\n",
      "Name: output, dtype: object [0 0 0 1 1 0 1 1 0 0] 1800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# transform labels into numbers\n",
    "labels2numbers = LabelEncoder()\n",
    "\n",
    "y = labels2numbers.fit_transform(data['output'])\n",
    "print(data['output'][:10], y[:10], len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the original names back, use `inverse_transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'pos', 'pos', 'neg', 'neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2numbers.inverse_transform([1,1,1,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Representing text\n",
    "\n",
    "First, we need to transform the texts into a matrix, where each row represents one text instance. The columns are the **features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bags of words\n",
    "\n",
    "The easiest way is to represent features is as a counts of all words in the text. It takes two steps:\n",
    "1. collect the counts for each word\n",
    "2. transform the individual counts into one big matrix\n",
    "\n",
    "The result is a matrix $X$ with one row for each instance, and one column for each word in the vocabulary.\n",
    "\n",
    "![Bag of words procedure](bow.png)\n",
    "\n",
    "We can use the `TfidfVectorizer` object to get the weighted frequency of each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 69016)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0.001, max_df=0.75, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(data.clean_text)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trasnlate back and forth between columns and words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3819"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barely scratch'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[4191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverse'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# [vectorizer.get_feature_names()[f] for f in (X.sum(axis=1)).A1.argsort()[:10]]\n",
    "vectorizer.get_feature_names()[1244]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often that word is in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.input.str.contains('bad')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Learning a classification model\n",
    "\n",
    "A classification model is simply a function that takes a text representation as input, and returns an output label.\n",
    "\n",
    "Inside that function is normally a set of weights. By multiplying the weight vector with the input vector, we get the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.1: Fitting a model\n",
    "\n",
    "Fitting a model is the process of finding the right weights to map the training inputs to the training outputs. Fitting to data in `sklearn` is easy: we use the `fit()` function, giving it the input matrix and output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 33.7 ms, total: 214 ms\n",
      "Wall time: 70.4 ms\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dirkhovy/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "%time classifier.fit(X, y)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting fitted model has coefficients (betas) for each word/feature in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 69016)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now examine the weights/coefficients/betas for the individual words (note that each word has an ID):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad -3.461779370057597\n"
     ]
    }
   ],
   "source": [
    "k = vectorizer.vocabulary_['bad'] # column position for the word\n",
    "print(vectorizer.get_feature_names()[k], classifier.coef_[0, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: in a two-class problem, our coefficents are in a vector: positive values indicate the positive class, negative values the other class.\n",
    "In a multi-class problem, we have one **row** of coefficients for each class: positive values indicate that this feature contributes to the class, negative values indicate that it contributes to other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Evaluating models\n",
    "\n",
    "Having a model is great, but how well does it do? Can it classify what it has seen? We need a way to estimate how well the model will work on new data.\n",
    "\n",
    "We need a metric to measure performance and a way to simulate new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.1: Metrics\n",
    "\n",
    "We use three measure:\n",
    "1. precision\n",
    "2. recall\n",
    "3. F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures how many of our model's predictions were correct. We divide the number of true positives by the number of all positives\n",
    "\n",
    "$$\n",
    "p = \\frac{tp}{tp+fp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "Recall measures how many of the correct answers in the data our model managed to find. We divide the number of true positives by the number of true positives (the instances our model got) and false negatives (the instances our model *should* have gotten)\n",
    "\n",
    "$$\n",
    "r = \\frac{tp}{tp+fn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1\n",
    "\n",
    "A model that classified everything as, say, \"positive\" would get a perfect recall (it does, after all, find all positive examples). However, such a model would obviously be useless, since its precision is bad.\n",
    "\n",
    "We want to balance the two against each other. F1 does exactly that, by taking the harmonic mean.\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{p\\cdot r}{p+r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Luckily, all of these metrics are implemented in `sklearn`. All we have to provide are the predictions of our model, and the actual correct answers (called the *gold standard*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2: Cross-validation\n",
    "\n",
    "How do we measure performance on new data, if we don't know what the correct outputs for those new data points are?\n",
    "\n",
    "In **$k$-fold cross-validation**, we simulate new data, by fitting our model on parts of the data, and evaluating on other. We can thereby measure the performance on the held-out part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we have now reduced the amount of data we used to fit the data. In order to address this, we simply repeat the process $k$ times.\n",
    "We separate the data into $k$ parts, fit the model on $k-1$ parts, and evaluate on the $k$th part. In the end, we have performance scores from $k$ models. The average of them tells us how well the model would work on new data.\n",
    "\n",
    "![3-fold cross-validation](3foldCV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 with 2 folds for bag-of-words is 0.8116880391210359\n",
      "Training on 900.0 instances/fold, testing on 900.0\n",
      "\n",
      "F1 with 3 folds for bag-of-words is 0.8161274444898149\n",
      "Training on 1200.0 instances/fold, testing on 600.0\n",
      "\n",
      "F1 with 5 folds for bag-of-words is 0.8244532236617053\n",
      "Training on 1440.0 instances/fold, testing on 360.0\n",
      "\n",
      "F1 with 10 folds for bag-of-words is 0.8327954052079797\n",
      "Training on 1620.0 instances/fold, testing on 180.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for k in [2,3,5,10]:\n",
    "    cv = cross_val_score(LogisticRegression(), X, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\")\n",
    "    fold_size = X.shape[0]/k\n",
    "    \n",
    "    print(\"F1 with {} folds for bag-of-words is {}\".format(k, cv.mean()))\n",
    "    print(\"Training on {} instances/fold, testing on {}\".format(fold_size*(k-1), fold_size))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "So, is that performance good? Let's compare to a **baseline**, i.e., a null-hypothesis. The simplest one is that all instances belong to the most fereuqnt class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506111162553028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "print(cross_val_score(most_frequent, X, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    "See whether you can apply the previous steps to a new data sets, a description of wines. Choose any of the descriptor columns as target variable. The text is already preprocessed, to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "      <th>description_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>tremendous varietal wine hail be age year oak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>honor memory wine once make his mother tremend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>spend month new french oak incorporate fruit v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This re-named vineyard was formerly bottled as...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>re name vineyard be formerly bottle will find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The producer sources from two blocks of the vi...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>producer source block vineyard wine high eleva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From 18-year-old vines, this supple well-balan...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>old vine supple well balance effort blend flav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A standout even in this terrific lineup of 201...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>standout even terrific lineup release open bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>With its sophisticated mix of mineral, acid an...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>its sophisticated mix mineral acid tart fruit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>First made in 2006, this succulent luscious Ch...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>first make succulent luscious be all mineralit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This blockbuster, powerhouse of a wine suggest...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>blockbuster powerhouse wine suggest blueberry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nicely oaked blackberry, licorice, vanilla and...</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Tempranillo</td>\n",
       "      <td>oak blackberry licorice vanilla char aroma be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Coming from a seven-acre vineyard named after ...</td>\n",
       "      <td>France</td>\n",
       "      <td>Southwest France</td>\n",
       "      <td>Malbec</td>\n",
       "      <td>come acre vineyard name dovecote property be m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This fresh and lively medium-bodied wine is be...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>fresh lively medium bodied wine be beautifully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Heitz has made this stellar rosé from the rare...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Rosé</td>\n",
       "      <td>have make stellar rosé rare grape ruby grapefr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The apogee of this ambitious winery's white wi...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>apogee ambitious winery white wine effort bott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>San Jose-based producer Adam Comartin heads 1,...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>base producer head foot up mountain source fru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bergström has made a Shea designate since 2003...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>have make designate intent showcas pretty styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Focused and dense, this intense wine captures ...</td>\n",
       "      <td>US</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>focused dense intense wine capture essence rip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cranberry, baked rhubarb, anise and crushed sl...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>cranberry bake rhubarb anise crush slate aroma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This standout Rocks District wine brings earth...</td>\n",
       "      <td>US</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>standout wine bring earth shake aroma black ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Steely and perfumed, this wine sees only 20% n...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>steely perfume wine see only new french oak re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Underbrush, scorched earth, menthol and plum s...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Sangiovese</td>\n",
       "      <td>scorch earth menthol plum steep spirit be arom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The aromas entice with notes of wet stone, hon...</td>\n",
       "      <td>US</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>aroma entice note wet stone honeysuckle chamom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Forest floor, tilled soil, mature berry and a ...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Sangiovese</td>\n",
       "      <td>forest floor till soil mature berry whiff new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aromas of forest floor, violet, red berry and ...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Sangiovese</td>\n",
       "      <td>forest floor violet red berry whiff dark bakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>This has a charming nose that boasts rose, vio...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Sangiovese</td>\n",
       "      <td>have charming nose that boast rise violet red ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>This bright, savory wine delivers aromas and f...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>bright savory wine deliver aroma flavor juicy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aromas of dark-skinned berry, rose and wild he...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>dark skinned berry rise wild herb lead nose ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Dark in color and in flavor profile, this medi...</td>\n",
       "      <td>France</td>\n",
       "      <td>Rhône Valley</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>dark color flavor profile medium bodied boast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A blend of 90% Sangiovese and 10% Canaiolo, th...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>blend open aroma violet mature plum espresso w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104607</th>\n",
       "      <td>Everything is front-end in this Chardonnay. Th...</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Colchagua Valley</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>everything be front end nose have plenty tropi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104608</th>\n",
       "      <td>A heavy wine, atypical of the appellation, whi...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>heavy wine atypical appellation which usually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104609</th>\n",
       "      <td>A coppery colored, off-dry-to-frankly-sweet wi...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>coppery color off dry frankly sweet wine that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104610</th>\n",
       "      <td>Here's a nice everyday drinking wine with some...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>here be nice everyday drink wine real richness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104611</th>\n",
       "      <td>A soft, round quaffer filled with warmth. Slig...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Merlot</td>\n",
       "      <td>soft round quaffer fill warmth slightly cooked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104612</th>\n",
       "      <td>A bizarre style of wine. The aromas are Port-l...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>bizarre style wine aroma be like old carameliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104613</th>\n",
       "      <td>A light, earthy wine, with violet, berry and t...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Merlot</td>\n",
       "      <td>light earthy wine violet berry tea flavor shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104614</th>\n",
       "      <td>Some raspberry fruit in the aroma, but things ...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>raspberry fruit aroma thing turn rapidly earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104615</th>\n",
       "      <td>This lovely wine, a Monopole, is already showi...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>lovely wine be already show complex fruit flav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104616</th>\n",
       "      <td>Rion holds back on the new oak, letting the pu...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>rion hold back new oak let pure supple fruit s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104617</th>\n",
       "      <td>Another premier cru from Michel Gros, this one...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>premier cru one not as elegant fruity instead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104618</th>\n",
       "      <td>This is a lovely, fragrant Burgundy, with a sm...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>be lovely fragrant smoky cherry scented nose l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104619</th>\n",
       "      <td>Scents of graham cracker and malted milk choco...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>scent graham cracker malt milk chocolate tickl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104620</th>\n",
       "      <td>This needs a good bit of breathing time, then ...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>need good bit breathing time then begin reveal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104621</th>\n",
       "      <td>The nose is dominated by the attractive scents...</td>\n",
       "      <td>France</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>nose be dominate attractive scent new french o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104622</th>\n",
       "      <td>Inky and rustic, yet in a refined manner. This...</td>\n",
       "      <td>France</td>\n",
       "      <td>Rhône Valley</td>\n",
       "      <td>Rhône-style Red Blend</td>\n",
       "      <td>inky rustic refined manner be knock juice like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104623</th>\n",
       "      <td>Decades ago, Beringer’s then-winemaker Myron N...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>decade ago then winemaker create artificial me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104624</th>\n",
       "      <td>An impressive wine that presents a full bouque...</td>\n",
       "      <td>US</td>\n",
       "      <td>California</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>impressive wine that present full bouquet brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104625</th>\n",
       "      <td>Light and elegant, this spicy, lively wine is ...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>light elegant spicy lively wine be gentle joy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104626</th>\n",
       "      <td>Jacquart makes a full-bodied, ripe style of Ch...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>make full bodied ripe style tiny effusive bubb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104627</th>\n",
       "      <td>This classy example opens with a very floral n...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>classy example open very floral nose almost fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104628</th>\n",
       "      <td>Rich and mature aromas of smoke, earth and her...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Northeastern Italy</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>rich mature aroma smoke earth herb announce su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104629</th>\n",
       "      <td>Shows some older notes: a bouquet of toasted w...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>show old note bouquet toast white bread cinnam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104630</th>\n",
       "      <td>Rich and toasty, with tiny bubbles. The bouque...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>rich toasty tiny bubble bouquet be lace honey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104631</th>\n",
       "      <td>Really fine for a low-acid vintage, there's an...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>really fine low acid vintage there be intrigui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104632</th>\n",
       "      <td>Many people feel Fiano represents southern Ita...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Southern Italy</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>many people feel represent southern most promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104633</th>\n",
       "      <td>Offers an intriguing nose with ginger, lime an...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>offer intriguing nose ginger lime floral eleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104634</th>\n",
       "      <td>This classic example comes from a cru vineyard...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Southern Italy</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>classic example come cru vineyard call which b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104635</th>\n",
       "      <td>A perfect salmon shade, with scents of peaches...</td>\n",
       "      <td>France</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>perfect salmon shade scent peach cherry toast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104636</th>\n",
       "      <td>More Pinot Grigios should taste like this. A r...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Northeastern Italy</td>\n",
       "      <td>Pinot Grigio</td>\n",
       "      <td>more pinot should taste rich pear like nose be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104637 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description country  \\\n",
       "0       This tremendous 100% varietal wine hails from ...      US   \n",
       "1       Mac Watson honors the memory of a wine once ma...      US   \n",
       "2       This spent 20 months in 30% new French oak, an...      US   \n",
       "3       This re-named vineyard was formerly bottled as...      US   \n",
       "4       The producer sources from two blocks of the vi...      US   \n",
       "5       From 18-year-old vines, this supple well-balan...      US   \n",
       "6       A standout even in this terrific lineup of 201...      US   \n",
       "7       With its sophisticated mix of mineral, acid an...      US   \n",
       "8       First made in 2006, this succulent luscious Ch...      US   \n",
       "9       This blockbuster, powerhouse of a wine suggest...      US   \n",
       "10      Nicely oaked blackberry, licorice, vanilla and...   Spain   \n",
       "11      Coming from a seven-acre vineyard named after ...  France   \n",
       "12      This fresh and lively medium-bodied wine is be...      US   \n",
       "13      Heitz has made this stellar rosé from the rare...      US   \n",
       "14      The apogee of this ambitious winery's white wi...      US   \n",
       "15      San Jose-based producer Adam Comartin heads 1,...      US   \n",
       "16      Bergström has made a Shea designate since 2003...      US   \n",
       "17      Focused and dense, this intense wine captures ...      US   \n",
       "18      Cranberry, baked rhubarb, anise and crushed sl...      US   \n",
       "19      This standout Rocks District wine brings earth...      US   \n",
       "20      Steely and perfumed, this wine sees only 20% n...      US   \n",
       "21      Underbrush, scorched earth, menthol and plum s...   Italy   \n",
       "22      The aromas entice with notes of wet stone, hon...      US   \n",
       "23      Forest floor, tilled soil, mature berry and a ...   Italy   \n",
       "24      Aromas of forest floor, violet, red berry and ...   Italy   \n",
       "25      This has a charming nose that boasts rose, vio...   Italy   \n",
       "26      This bright, savory wine delivers aromas and f...   Italy   \n",
       "27      Aromas of dark-skinned berry, rose and wild he...   Italy   \n",
       "28      Dark in color and in flavor profile, this medi...  France   \n",
       "29      A blend of 90% Sangiovese and 10% Canaiolo, th...   Italy   \n",
       "...                                                   ...     ...   \n",
       "104607  Everything is front-end in this Chardonnay. Th...   Chile   \n",
       "104608  A heavy wine, atypical of the appellation, whi...      US   \n",
       "104609  A coppery colored, off-dry-to-frankly-sweet wi...      US   \n",
       "104610  Here's a nice everyday drinking wine with some...      US   \n",
       "104611  A soft, round quaffer filled with warmth. Slig...      US   \n",
       "104612  A bizarre style of wine. The aromas are Port-l...      US   \n",
       "104613  A light, earthy wine, with violet, berry and t...      US   \n",
       "104614  Some raspberry fruit in the aroma, but things ...      US   \n",
       "104615  This lovely wine, a Monopole, is already showi...  France   \n",
       "104616  Rion holds back on the new oak, letting the pu...  France   \n",
       "104617  Another premier cru from Michel Gros, this one...  France   \n",
       "104618  This is a lovely, fragrant Burgundy, with a sm...  France   \n",
       "104619  Scents of graham cracker and malted milk choco...  France   \n",
       "104620  This needs a good bit of breathing time, then ...  France   \n",
       "104621  The nose is dominated by the attractive scents...  France   \n",
       "104622  Inky and rustic, yet in a refined manner. This...  France   \n",
       "104623  Decades ago, Beringer’s then-winemaker Myron N...      US   \n",
       "104624  An impressive wine that presents a full bouque...      US   \n",
       "104625  Light and elegant, this spicy, lively wine is ...  France   \n",
       "104626  Jacquart makes a full-bodied, ripe style of Ch...  France   \n",
       "104627  This classy example opens with a very floral n...  France   \n",
       "104628  Rich and mature aromas of smoke, earth and her...   Italy   \n",
       "104629  Shows some older notes: a bouquet of toasted w...  France   \n",
       "104630  Rich and toasty, with tiny bubbles. The bouque...  France   \n",
       "104631  Really fine for a low-acid vintage, there's an...  France   \n",
       "104632  Many people feel Fiano represents southern Ita...   Italy   \n",
       "104633  Offers an intriguing nose with ginger, lime an...  France   \n",
       "104634  This classic example comes from a cru vineyard...   Italy   \n",
       "104635  A perfect salmon shade, with scents of peaches...  France   \n",
       "104636  More Pinot Grigios should taste like this. A r...   Italy   \n",
       "\n",
       "                  province                variety  \\\n",
       "0               California     Cabernet Sauvignon   \n",
       "1               California        Sauvignon Blanc   \n",
       "2                   Oregon             Pinot Noir   \n",
       "3                   Oregon             Pinot Noir   \n",
       "4               California             Pinot Noir   \n",
       "5                   Oregon             Pinot Noir   \n",
       "6                   Oregon             Pinot Noir   \n",
       "7                   Oregon             Pinot Noir   \n",
       "8                   Oregon             Chardonnay   \n",
       "9               California     Cabernet Sauvignon   \n",
       "10          Northern Spain            Tempranillo   \n",
       "11        Southwest France                 Malbec   \n",
       "12              California             Pinot Noir   \n",
       "13              California                   Rosé   \n",
       "14              California             Chardonnay   \n",
       "15              California             Pinot Noir   \n",
       "16                  Oregon             Pinot Noir   \n",
       "17                  Oregon             Pinot Noir   \n",
       "18              California             Pinot Noir   \n",
       "19              Washington                  Syrah   \n",
       "20              California             Chardonnay   \n",
       "21                 Tuscany             Sangiovese   \n",
       "22              Washington             Chardonnay   \n",
       "23                 Tuscany             Sangiovese   \n",
       "24                 Tuscany             Sangiovese   \n",
       "25                 Tuscany             Sangiovese   \n",
       "26                 Tuscany              Red Blend   \n",
       "27                 Tuscany              Red Blend   \n",
       "28            Rhône Valley                  Syrah   \n",
       "29                 Tuscany              Red Blend   \n",
       "...                    ...                    ...   \n",
       "104607    Colchagua Valley             Chardonnay   \n",
       "104608          California             Pinot Noir   \n",
       "104609          California              Zinfandel   \n",
       "104610          California             Chardonnay   \n",
       "104611          California                 Merlot   \n",
       "104612          California             Pinot Noir   \n",
       "104613          California                 Merlot   \n",
       "104614          California             Pinot Noir   \n",
       "104615            Burgundy             Pinot Noir   \n",
       "104616            Burgundy             Pinot Noir   \n",
       "104617            Burgundy             Pinot Noir   \n",
       "104618            Burgundy             Pinot Noir   \n",
       "104619            Burgundy             Pinot Noir   \n",
       "104620            Burgundy             Pinot Noir   \n",
       "104621            Burgundy             Pinot Noir   \n",
       "104622        Rhône Valley  Rhône-style Red Blend   \n",
       "104623          California            White Blend   \n",
       "104624          California        Champagne Blend   \n",
       "104625           Champagne        Champagne Blend   \n",
       "104626           Champagne        Champagne Blend   \n",
       "104627           Champagne        Champagne Blend   \n",
       "104628  Northeastern Italy        Champagne Blend   \n",
       "104629           Champagne        Champagne Blend   \n",
       "104630           Champagne        Champagne Blend   \n",
       "104631           Champagne        Champagne Blend   \n",
       "104632      Southern Italy            White Blend   \n",
       "104633           Champagne        Champagne Blend   \n",
       "104634      Southern Italy            White Blend   \n",
       "104635           Champagne        Champagne Blend   \n",
       "104636  Northeastern Italy           Pinot Grigio   \n",
       "\n",
       "                                      description_cleaned  \n",
       "0       tremendous varietal wine hail be age year oak ...  \n",
       "1       honor memory wine once make his mother tremend...  \n",
       "2       spend month new french oak incorporate fruit v...  \n",
       "3       re name vineyard be formerly bottle will find ...  \n",
       "4       producer source block vineyard wine high eleva...  \n",
       "5       old vine supple well balance effort blend flav...  \n",
       "6       standout even terrific lineup release open bur...  \n",
       "7       its sophisticated mix mineral acid tart fruit ...  \n",
       "8       first make succulent luscious be all mineralit...  \n",
       "9       blockbuster powerhouse wine suggest blueberry ...  \n",
       "10      oak blackberry licorice vanilla char aroma be ...  \n",
       "11      come acre vineyard name dovecote property be m...  \n",
       "12      fresh lively medium bodied wine be beautifully...  \n",
       "13      have make stellar rosé rare grape ruby grapefr...  \n",
       "14      apogee ambitious winery white wine effort bott...  \n",
       "15      base producer head foot up mountain source fru...  \n",
       "16      have make designate intent showcas pretty styl...  \n",
       "17      focused dense intense wine capture essence rip...  \n",
       "18      cranberry bake rhubarb anise crush slate aroma...  \n",
       "19      standout wine bring earth shake aroma black ol...  \n",
       "20      steely perfume wine see only new french oak re...  \n",
       "21      scorch earth menthol plum steep spirit be arom...  \n",
       "22      aroma entice note wet stone honeysuckle chamom...  \n",
       "23      forest floor till soil mature berry whiff new ...  \n",
       "24      forest floor violet red berry whiff dark bakin...  \n",
       "25      have charming nose that boast rise violet red ...  \n",
       "26      bright savory wine deliver aroma flavor juicy ...  \n",
       "27      dark skinned berry rise wild herb lead nose ro...  \n",
       "28      dark color flavor profile medium bodied boast ...  \n",
       "29      blend open aroma violet mature plum espresso w...  \n",
       "...                                                   ...  \n",
       "104607  everything be front end nose have plenty tropi...  \n",
       "104608  heavy wine atypical appellation which usually ...  \n",
       "104609  coppery color off dry frankly sweet wine that ...  \n",
       "104610  here be nice everyday drink wine real richness...  \n",
       "104611  soft round quaffer fill warmth slightly cooked...  \n",
       "104612  bizarre style wine aroma be like old carameliz...  \n",
       "104613  light earthy wine violet berry tea flavor shar...  \n",
       "104614  raspberry fruit aroma thing turn rapidly earth...  \n",
       "104615  lovely wine be already show complex fruit flav...  \n",
       "104616  rion hold back new oak let pure supple fruit s...  \n",
       "104617  premier cru one not as elegant fruity instead ...  \n",
       "104618  be lovely fragrant smoky cherry scented nose l...  \n",
       "104619  scent graham cracker malt milk chocolate tickl...  \n",
       "104620  need good bit breathing time then begin reveal...  \n",
       "104621  nose be dominate attractive scent new french o...  \n",
       "104622  inky rustic refined manner be knock juice like...  \n",
       "104623  decade ago then winemaker create artificial me...  \n",
       "104624  impressive wine that present full bouquet brea...  \n",
       "104625  light elegant spicy lively wine be gentle joy ...  \n",
       "104626  make full bodied ripe style tiny effusive bubb...  \n",
       "104627  classy example open very floral nose almost fl...  \n",
       "104628  rich mature aroma smoke earth herb announce su...  \n",
       "104629  show old note bouquet toast white bread cinnam...  \n",
       "104630  rich toasty tiny bubble bouquet be lace honey ...  \n",
       "104631  really fine low acid vintage there be intrigui...  \n",
       "104632  many people feel represent southern most promi...  \n",
       "104633  offer intriguing nose ginger lime floral eleme...  \n",
       "104634  classic example come cru vineyard call which b...  \n",
       "104635  perfect salmon shade scent peach cherry toast ...  \n",
       "104636  more pinot should taste rich pear like nose be...  \n",
       "\n",
       "[104637 rows x 5 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('wine.csv', encoding='utf8')\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 6 Classifying new data\n",
    "\n",
    "Classifying new (**held-out**) data is called **prediction**. We reuse the weights we have learned before on a new data matrix to predict the new outcomes.\n",
    "Important: the new data needs to have the same number of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robert redford ' s a river runs through it is ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if the 70 ' s nostalgia didn ' t make you feel...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you think that these people only exist in the ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" knock off \" is exactly that : a cheap knock ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brian depalma needs a hit * really * badly . s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input output\n",
       "0  robert redford ' s a river runs through it is ...    pos\n",
       "1  if the 70 ' s nostalgia didn ' t make you feel...    neg\n",
       "2  you think that these people only exist in the ...    neg\n",
       "3  \" knock off \" is exactly that : a cheap knock ...    neg\n",
       "4  brian depalma needs a hit * really * badly . s...    pos"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in new data set\n",
    "new_data = pd.read_csv('sa_test.csv')\n",
    "print(len(new_data))\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to clean it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 1.91 s, total: 17.6 s\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%time new_data['clean_text'] = new_data.input.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how well we do on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
      " 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# transform text into word counts\n",
    "# IMPORTANT: use same vectorizer we fit on training data to create vectors!\n",
    "new_X = vectorizer.transform(new_data.clean_text)\n",
    "\n",
    "# use the old classifier to predict and evaluate\n",
    "new_predictions = classifier.predict(new_X)\n",
    "print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can also predict the probabilities of belonging to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31108778 0.68891222]\n",
      " [0.55471719 0.44528281]\n",
      " [0.53620949 0.46379051]\n",
      " [0.7502412  0.2497588 ]\n",
      " [0.35878419 0.64121581]\n",
      " [0.6253547  0.3746453 ]\n",
      " [0.53285443 0.46714557]\n",
      " [0.53272824 0.46727176]\n",
      " [0.639782   0.360218  ]\n",
      " [0.50654981 0.49345019]\n",
      " [0.39081759 0.60918241]\n",
      " [0.27007928 0.72992072]\n",
      " [0.36192997 0.63807003]\n",
      " [0.62977267 0.37022733]\n",
      " [0.366374   0.633626  ]\n",
      " [0.34421595 0.65578405]\n",
      " [0.30788486 0.69211514]\n",
      " [0.35909185 0.64090815]\n",
      " [0.48506903 0.51493097]\n",
      " [0.46259152 0.53740848]\n",
      " [0.72499626 0.27500374]\n",
      " [0.47746115 0.52253885]\n",
      " [0.35432731 0.64567269]\n",
      " [0.39762716 0.60237284]\n",
      " [0.50426902 0.49573098]\n",
      " [0.56830208 0.43169792]\n",
      " [0.68846081 0.31153919]\n",
      " [0.67481961 0.32518039]\n",
      " [0.45491445 0.54508555]\n",
      " [0.53817586 0.46182414]\n",
      " [0.30234197 0.69765803]\n",
      " [0.63989832 0.36010168]\n",
      " [0.70779382 0.29220618]\n",
      " [0.80626669 0.19373331]\n",
      " [0.64977241 0.35022759]\n",
      " [0.72614584 0.27385416]\n",
      " [0.3072466  0.6927534 ]\n",
      " [0.5791168  0.4208832 ]\n",
      " [0.59156876 0.40843124]\n",
      " [0.63534134 0.36465866]\n",
      " [0.66326988 0.33673012]\n",
      " [0.67488763 0.32511237]\n",
      " [0.72706722 0.27293278]\n",
      " [0.73420711 0.26579289]\n",
      " [0.39686168 0.60313832]\n",
      " [0.41954471 0.58045529]\n",
      " [0.70426891 0.29573109]\n",
      " [0.58739344 0.41260656]\n",
      " [0.65560337 0.34439663]\n",
      " [0.68156901 0.31843099]\n",
      " [0.3577818  0.6422182 ]\n",
      " [0.26335983 0.73664017]\n",
      " [0.62231416 0.37768584]\n",
      " [0.38600314 0.61399686]\n",
      " [0.51194573 0.48805427]\n",
      " [0.64692002 0.35307998]\n",
      " [0.46368927 0.53631073]\n",
      " [0.28668193 0.71331807]\n",
      " [0.61627314 0.38372686]\n",
      " [0.55522628 0.44477372]\n",
      " [0.7267492  0.2732508 ]\n",
      " [0.31651864 0.68348136]\n",
      " [0.59098665 0.40901335]\n",
      " [0.60896837 0.39103163]\n",
      " [0.3763535  0.6236465 ]\n",
      " [0.42747362 0.57252638]\n",
      " [0.55787056 0.44212944]\n",
      " [0.51906387 0.48093613]\n",
      " [0.58866944 0.41133056]\n",
      " [0.34608119 0.65391881]\n",
      " [0.47098871 0.52901129]\n",
      " [0.44218615 0.55781385]\n",
      " [0.66258394 0.33741606]\n",
      " [0.270418   0.729582  ]\n",
      " [0.46675067 0.53324933]\n",
      " [0.5424298  0.4575702 ]\n",
      " [0.51480941 0.48519059]\n",
      " [0.60021705 0.39978295]\n",
      " [0.6130465  0.3869535 ]\n",
      " [0.45096156 0.54903844]\n",
      " [0.37759356 0.62240644]\n",
      " [0.26746245 0.73253755]\n",
      " [0.40703962 0.59296038]\n",
      " [0.25924644 0.74075356]\n",
      " [0.44108428 0.55891572]\n",
      " [0.3653803  0.6346197 ]\n",
      " [0.64129912 0.35870088]\n",
      " [0.80134938 0.19865062]\n",
      " [0.5090975  0.4909025 ]\n",
      " [0.57197776 0.42802224]\n",
      " [0.56872945 0.43127055]\n",
      " [0.4868102  0.5131898 ]\n",
      " [0.49742086 0.50257914]\n",
      " [0.36937264 0.63062736]\n",
      " [0.29764862 0.70235138]\n",
      " [0.53413156 0.46586844]\n",
      " [0.65654894 0.34345106]\n",
      " [0.31500625 0.68499375]\n",
      " [0.58909028 0.41090972]\n",
      " [0.39549765 0.60450235]\n",
      " [0.37783436 0.62216564]\n",
      " [0.26254619 0.73745381]\n",
      " [0.54670123 0.45329877]\n",
      " [0.57106598 0.42893402]\n",
      " [0.53814185 0.46185815]\n",
      " [0.54462653 0.45537347]\n",
      " [0.31890732 0.68109268]\n",
      " [0.55278348 0.44721652]\n",
      " [0.52050724 0.47949276]\n",
      " [0.67324189 0.32675811]\n",
      " [0.56114923 0.43885077]\n",
      " [0.81862495 0.18137505]\n",
      " [0.51653895 0.48346105]\n",
      " [0.41406181 0.58593819]\n",
      " [0.37599385 0.62400615]\n",
      " [0.66241408 0.33758592]\n",
      " [0.48594019 0.51405981]\n",
      " [0.44152054 0.55847946]\n",
      " [0.55027974 0.44972026]\n",
      " [0.41320588 0.58679412]\n",
      " [0.61498729 0.38501271]\n",
      " [0.61991767 0.38008233]\n",
      " [0.33019744 0.66980256]\n",
      " [0.46887129 0.53112871]\n",
      " [0.40361092 0.59638908]\n",
      " [0.57448504 0.42551496]\n",
      " [0.52059246 0.47940754]\n",
      " [0.36810971 0.63189029]\n",
      " [0.47794138 0.52205862]\n",
      " [0.67407712 0.32592288]\n",
      " [0.45142567 0.54857433]\n",
      " [0.84435123 0.15564877]\n",
      " [0.78340433 0.21659567]\n",
      " [0.24300377 0.75699623]\n",
      " [0.32663242 0.67336758]\n",
      " [0.25826822 0.74173178]\n",
      " [0.44661457 0.55338543]\n",
      " [0.43160689 0.56839311]\n",
      " [0.55591923 0.44408077]\n",
      " [0.54201905 0.45798095]\n",
      " [0.652658   0.347342  ]\n",
      " [0.64243288 0.35756712]\n",
      " [0.65180629 0.34819371]\n",
      " [0.54999337 0.45000663]\n",
      " [0.46309627 0.53690373]\n",
      " [0.23983747 0.76016253]\n",
      " [0.56908792 0.43091208]\n",
      " [0.66454725 0.33545275]\n",
      " [0.38147045 0.61852955]\n",
      " [0.42591647 0.57408353]\n",
      " [0.37145609 0.62854391]\n",
      " [0.51867894 0.48132106]\n",
      " [0.37649847 0.62350153]\n",
      " [0.43350879 0.56649121]\n",
      " [0.65592614 0.34407386]\n",
      " [0.26273051 0.73726949]\n",
      " [0.62500739 0.37499261]\n",
      " [0.44135859 0.55864141]\n",
      " [0.37506069 0.62493931]\n",
      " [0.62441519 0.37558481]\n",
      " [0.53928276 0.46071724]\n",
      " [0.36808945 0.63191055]\n",
      " [0.27287275 0.72712725]\n",
      " [0.50173997 0.49826003]\n",
      " [0.66896624 0.33103376]\n",
      " [0.44416068 0.55583932]\n",
      " [0.61552828 0.38447172]\n",
      " [0.73255294 0.26744706]\n",
      " [0.60129331 0.39870669]\n",
      " [0.53142631 0.46857369]\n",
      " [0.56239138 0.43760862]\n",
      " [0.51207658 0.48792342]\n",
      " [0.32495615 0.67504385]\n",
      " [0.38689029 0.61310971]\n",
      " [0.39910392 0.60089608]\n",
      " [0.68827848 0.31172152]\n",
      " [0.49341456 0.50658544]\n",
      " [0.51971593 0.48028407]\n",
      " [0.55100028 0.44899972]\n",
      " [0.42712225 0.57287775]\n",
      " [0.57072949 0.42927051]\n",
      " [0.45048222 0.54951778]\n",
      " [0.3709303  0.6290697 ]\n",
      " [0.60104482 0.39895518]\n",
      " [0.69499912 0.30500088]\n",
      " [0.595289   0.404711  ]\n",
      " [0.56375595 0.43624405]\n",
      " [0.66227543 0.33772457]\n",
      " [0.21528933 0.78471067]\n",
      " [0.4673536  0.5326464 ]\n",
      " [0.33052281 0.66947719]\n",
      " [0.47415715 0.52584285]\n",
      " [0.69681108 0.30318892]\n",
      " [0.36343241 0.63656759]\n",
      " [0.573399   0.426601  ]\n",
      " [0.48061612 0.51938388]\n",
      " [0.5647077  0.4352923 ]\n",
      " [0.51527492 0.48472508]\n",
      " [0.5182421  0.4817579 ]\n",
      " [0.39186638 0.60813362]]\n"
     ]
    }
   ],
   "source": [
    "new_probabilities = classifier.predict_proba(new_X)\n",
    "print(new_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance (=row), we get a probability distribution over the classes (=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.1 Regularization\n",
    "\n",
    "Typically, performance is lower on unseen data, because our model **overfit** the training data: it expects the new data to look *exactly* the same as the training data. That is almost never true.\n",
    "\n",
    "In order to prevent the model from overfitting, we need to **regularize** it. Essentially, we make it harder to learn the training data.\n",
    "\n",
    "A simple example of regularization is to \"corrupt\" the training data by adding a little bit of noise to each training instance. Since the noise is irregular, it becomes harder for the model to learn any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-1f2ccac2e7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_regularized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_regularized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1_micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mrandom\u001b[0;34m(m, n, density, format, dtype, random_state, data_rvs)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mdata_rvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.sparse import random\n",
    "\n",
    "num_instances, num_features = X.shape\n",
    "\n",
    "for i in range(5):\n",
    "    X_regularized = X + random(num_instances, num_features, density=0.01)\n",
    "\n",
    "    print(cross_val_score(LogisticRegression(), X_regularized, y=y, cv=k, n_jobs=-1, scoring=\"f1_micro\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you run the previous cell several times, you see different results (it gets even more varied if you change `density`). This variation arises because we add **random** noise. Not good...\n",
    "\n",
    "Instead, it makes sense to force the model to spread the weights more evenly over all features, rather than bet on a few feature, which mighht not be present in future data.\n",
    "\n",
    "We can do this by training the model with the `C` parameter. The default is `1`. Lower values mean stricter regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-CV on train at C=50: 0.8483421725647746\n",
      "\n",
      "5-CV on train at C=20: 0.8500119428219183\n",
      "\n",
      "5-CV on train at C=10: 0.8494594822833854\n",
      "\n",
      "5-CV on train at C=1.0: 0.8244532236617053\n",
      "\n",
      "5-CV on train at C=0.5: 0.8172356182446538\n",
      "\n",
      "5-CV on train at C=0.1: 0.7855657622529667\n",
      "\n",
      "5-CV on train at C=0.05: 0.7355763890496412\n",
      "\n",
      "5-CV on train at C=0.01: 0.5083364497839918\n",
      "\n",
      "best C parameter: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "best_c = None\n",
    "best_f1_score = 0.0\n",
    "for c in [50, 20, 10, 1.0, 0.5, 0.1, 0.05, 0.01]:\n",
    "    clf = LogisticRegression(C=c)\n",
    "    cv_reg = cross_val_score(clf, X, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\").mean()\n",
    "\n",
    "    print(\"5-CV on train at C={}: {}\".format(c, cv_reg.mean()))\n",
    "    print()\n",
    "\n",
    "    if cv_reg > best_f1_score:\n",
    "        best_f1_score = cv_reg\n",
    "        best_c = c\n",
    "        \n",
    "print(\"best C parameter: {}\".format(best_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better features = better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now have **a lot** of features! More than we have actual examples...\n",
    "\n",
    "Not all of them will be helpful, though. Let's select the top 1500 based on how well they predict they outcome of the training data.\n",
    "\n",
    "We use two libraries from `sklearn`, `SelectKBest` (the selection algorithm) and `chi2` (the selection criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=1500).fit(X, y)\n",
    "X_sel = selector.transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how well this new representation performs, by looking at the 5-fold cross-validation. We keep the best regularization value from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-CV on train: 0.8972358154341039\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=best_c)\n",
    "\n",
    "cv_reg = cross_val_score(clf, X_sel, y=y, cv=5, n_jobs=-1, scoring=\"f1_micro\")\n",
    "print(\"5-CV on train: {}\".format(cv_reg.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not too bad! We have handily beaten our previous best! Let's fit a classifier on the whole data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dirkhovy/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_sel, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's apply it to the held-out data set. \n",
    "We need to \n",
    "* vectorize the data with our vectorizer from before (otherwise, we get different features)\n",
    "* select the top features (using our previously fitted selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1500)\n"
     ]
    }
   ],
   "source": [
    "# select features for new data\n",
    "new_X_sel = selector.transform(new_X)\n",
    "print(new_X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can use our new classifier to predict the new data labels, and compare them to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robert redford ' s a river runs through it is ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if the 70 ' s nostalgia didn ' t make you feel...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you think that these people only exist in the ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" knock off \" is exactly that : a cheap knock ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brian depalma needs a hit * really * badly . s...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attention moviegoers : you are about to enter ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it used to be that not just anyone could becom...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expand the final fifteen minutes of home alone...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>capsule : godawful \" comedy \" that ' s amazing...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drew barrymore is beginning to corner the mark...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when andy leaves for cowboy camp , his mother ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with more and more television shows having gay...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>there ' s good news and bad news about mulan ....</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what do you get when you rip - off good movies...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>whenever writer / director robert altman works...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>star wars : ? episode i -- the phantom menace ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>when _star wars_ came out some twenty years ag...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>carry on at your convenience is all about the ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>at the outset of swordfish , john travolta ' s...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>one of the most respected names in american in...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>you don ' t look at a ren ? magritte painting ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the bond series is an island in the film world...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the premise of wag the dog is so simple that i...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>when you go to the movies as much as i do , yo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>this film is based on the campy tv show from t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>when i ponder childhood memories past , one of...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>it ' s been hours since i returned from the mu...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>the best thing about , \" lake placid \" is that...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>blade is the movie that shows that wesley snip...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the working title for no looking back was long...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>the long kiss goodnight ( r ) meryl streep tri...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>the crown jewel of 1970 ' s irwin allen disast...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>anna and the king is at least the fourth film ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>this is the last carry on film with its almost...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>if he doesn = 92t watch out , mel gibson is in...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>welcome to your oh - so typical sequel . it tr...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>so , it ' s thirty years later , and oscar and...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>jean - luc picard ( patrick stewart ) and the ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>hello kids . today the movie studios want to t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>in recent years , harrison ford has been such ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>one of the first films of 1999 is this mtv pic...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>now that \" boogie nights \" has made disco resp...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>there was a lot riding on this movie . everyon...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>post - chasing amy , a slew of love - triangle...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>the happy bastard ' s quick movie review wild ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>the spy game is up . you can thank charlie ' s...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>there are those of us who think of leslie niel...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>last summer , a feature - length version of th...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>sometimes you just have to tip your hat to a f...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>mugshot ( director / writer / cinematographer ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>warning : this review contains some spoilers f...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>a silly film that tries to be a black comedy b...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>summer movies are , by nature , dumb affairs t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>robert redford is very good at playing charact...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>by starring in amy heckerling ' s \" clueless \"...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>i won \u0012 t even pretend that i have seen the ot...</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>the cartoon is way better . that ' s the botto...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>dr . alan grant ( sam neill , \" jurassic park ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>of course i knew this going in . why is it tha...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>rated : r for strong language , sexual dialogu...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input prediction truth\n",
       "0    robert redford ' s a river runs through it is ...        pos   pos\n",
       "1    if the 70 ' s nostalgia didn ' t make you feel...        neg   neg\n",
       "2    you think that these people only exist in the ...        neg   neg\n",
       "3    \" knock off \" is exactly that : a cheap knock ...        neg   neg\n",
       "4    brian depalma needs a hit * really * badly . s...        pos   pos\n",
       "5    attention moviegoers : you are about to enter ...        neg   neg\n",
       "6    it used to be that not just anyone could becom...        neg   neg\n",
       "7    expand the final fifteen minutes of home alone...        pos   pos\n",
       "8    capsule : godawful \" comedy \" that ' s amazing...        neg   neg\n",
       "9    drew barrymore is beginning to corner the mark...        pos   neg\n",
       "10   when andy leaves for cowboy camp , his mother ...        pos   pos\n",
       "11   with more and more television shows having gay...        pos   pos\n",
       "12   there ' s good news and bad news about mulan ....        pos   pos\n",
       "13   what do you get when you rip - off good movies...        neg   neg\n",
       "14   whenever writer / director robert altman works...        pos   pos\n",
       "15   star wars : ? episode i -- the phantom menace ...        pos   neg\n",
       "16   when _star wars_ came out some twenty years ag...        pos   pos\n",
       "17   carry on at your convenience is all about the ...        pos   neg\n",
       "18   at the outset of swordfish , john travolta ' s...        neg   neg\n",
       "19   one of the most respected names in american in...        neg   neg\n",
       "20   you don ' t look at a ren ? magritte painting ...        neg   neg\n",
       "21   the bond series is an island in the film world...        pos   pos\n",
       "22   the premise of wag the dog is so simple that i...        pos   pos\n",
       "23   when you go to the movies as much as i do , yo...        pos   pos\n",
       "24   this film is based on the campy tv show from t...        neg   pos\n",
       "25   when i ponder childhood memories past , one of...        neg   neg\n",
       "26   it ' s been hours since i returned from the mu...        neg   neg\n",
       "27   the best thing about , \" lake placid \" is that...        neg   neg\n",
       "28   blade is the movie that shows that wesley snip...        pos   pos\n",
       "29   the working title for no looking back was long...        neg   neg\n",
       "..                                                 ...        ...   ...\n",
       "170  the long kiss goodnight ( r ) meryl streep tri...        neg   pos\n",
       "171  the crown jewel of 1970 ' s irwin allen disast...        pos   neg\n",
       "172  anna and the king is at least the fourth film ...        pos   neg\n",
       "173  this is the last carry on film with its almost...        pos   neg\n",
       "174  if he doesn = 92t watch out , mel gibson is in...        pos   pos\n",
       "175  welcome to your oh - so typical sequel . it tr...        neg   neg\n",
       "176  so , it ' s thirty years later , and oscar and...        neg   neg\n",
       "177  jean - luc picard ( patrick stewart ) and the ...        neg   pos\n",
       "178  hello kids . today the movie studios want to t...        neg   neg\n",
       "179  in recent years , harrison ford has been such ...        pos   pos\n",
       "180  one of the first films of 1999 is this mtv pic...        neg   neg\n",
       "181  now that \" boogie nights \" has made disco resp...        pos   pos\n",
       "182  there was a lot riding on this movie . everyon...        pos   pos\n",
       "183  post - chasing amy , a slew of love - triangle...        neg   neg\n",
       "184  the happy bastard ' s quick movie review wild ...        neg   neg\n",
       "185  the spy game is up . you can thank charlie ' s...        neg   neg\n",
       "186  there are those of us who think of leslie niel...        neg   neg\n",
       "187  last summer , a feature - length version of th...        neg   neg\n",
       "188  sometimes you just have to tip your hat to a f...        pos   pos\n",
       "189  mugshot ( director / writer / cinematographer ...        neg   neg\n",
       "190  warning : this review contains some spoilers f...        pos   pos\n",
       "191  a silly film that tries to be a black comedy b...        pos   neg\n",
       "192  summer movies are , by nature , dumb affairs t...        neg   neg\n",
       "193  robert redford is very good at playing charact...        pos   pos\n",
       "194  by starring in amy heckerling ' s \" clueless \"...        neg   neg\n",
       "195  i won \u0012 t even pretend that i have seen the ot...        pos   neg\n",
       "196  the cartoon is way better . that ' s the botto...        neg   neg\n",
       "197  dr . alan grant ( sam neill , \" jurassic park ...        neg   neg\n",
       "198  of course i knew this going in . why is it tha...        neg   neg\n",
       "199  rated : r for strong language , sexual dialogu...        pos   pos\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions_regularized = clf.predict(new_X_sel)\n",
    "prediction_df = pd.DataFrame(data={'input': new_data['input'], 'prediction': labels2numbers.inverse_transform(new_predictions_regularized), 'truth':new_data['output']})\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drew barrymore is beginning to corner the market on playing the girl outside - the one who \\' s the awkward klutz or the spunky do - it - yourselfer ; the one who just doesn \\' t fit in with the others . she has perfected these characters in movies such as \" the wedding singer \" and , most notably , \" ever after . \" now she \\' s back , starring in what could be called a modern - day cinderella fable - \" never been kissed . \" you know it \\' s a fable because she plays a copy editor at a newspaper who has her own office as well as a secretary . trust me on this one , no copy editor has seen the inside of a private office since gutenberg ( and i don \\' t mean steve ) invented the printing press . the premise is simple . barrymore \\' s josie geller , at 25 the youngest copy editor ever to be hired by the chicago sun - times , is assigned to go undercover and return to high school to do an expose on what today \\' s teens are feeling and doing . josie ( she says she was named after the \\' 70s cartoon character ) was a geek in high school , so she jumps at the opportunity for a second chance . this time , she thinks , she will get it right and be accepted by the in - crowd . now , what kind of adult - with a good job and a successful career - would actually look forward to reliving the hell that was - and is - high school and adolescence . these are among the many problems that plague \" never been kissed . \" screenwriters abby kohn and marc silverstein cannot get a handle of josie . their script has her capriciously switching from confident adult to ditzy , blubbering woman - child at the least provocation . and the fact that an adult would put so much stock into trying to become tight with the vapid airheads who are supposedly south glen high school \\' s most popular girls leads you to question her maturity and mental stability . ok , so \" never been kissed \" is not a sociological expose of today \\' s high school scene . however , certain rules should apply to film , and one of those is consistency of character . among the movie \\' s problems is the ill - conceived conceit that josie would seriously strive to climb the high school food chain and , in the process , lose focus on her assignment . any competent editor would have tossed her ass out the door quicker than you could say \" get me rewrite . \" to be fair , barrymore is very appealing , but she is given very little to work with . she tries valiantly to get a firm grip on her character , but the script continually undermines her . barrymore comes off best in the physical comedy aspects of the script in which she tries to walk , talk and act like a cool , hip high schooler . otherwise , she is left foundering on a cliched sea of teen - age stereotypes and situations . \" never been kissed \" is entertaining and funny in fits and starts . it lacks consistency and a firm grasp on what it wants to accomplish . the movie \\' s main bright spot is provided by leelee sobieski as aldys , the outsider who befriends new student josie . josie sees a lot of her former self in aldys , yet still abandons her to hang out with the popular girls . real mature . and that is the main deficiency with \" never been kissed . \" it \\' s illogical , unrealistic , uneven and undemanding . it has some warm and humorous spots , but not enough to overcome its many obstacles .'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['input'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting insights\n",
    "\n",
    "In order to explore which features are most indicative, we need some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>bad</td>\n",
       "      <td>-12.740770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>waste</td>\n",
       "      <td>-8.606913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>suppose</td>\n",
       "      <td>-8.403378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>attempt</td>\n",
       "      <td>-8.137514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>boring</td>\n",
       "      <td>-7.687704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>plot</td>\n",
       "      <td>-7.204338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>-6.920251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>poor</td>\n",
       "      <td>-6.274391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>mess</td>\n",
       "      <td>-5.804681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>-5.763368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>dull</td>\n",
       "      <td>-5.746076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>maybe</td>\n",
       "      <td>-5.607562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>fail</td>\n",
       "      <td>-5.531238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>awful</td>\n",
       "      <td>-5.514160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cheap</td>\n",
       "      <td>-5.281592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-5.255674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>guess</td>\n",
       "      <td>-5.236280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-5.129538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>flat</td>\n",
       "      <td>-5.107622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>potential</td>\n",
       "      <td>-5.026281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-4.976049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>script</td>\n",
       "      <td>-4.918184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>embarrassing</td>\n",
       "      <td>-4.835148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>lame</td>\n",
       "      <td>-4.776236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>tedious</td>\n",
       "      <td>-4.752035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>material</td>\n",
       "      <td>-4.718103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>idea</td>\n",
       "      <td>-4.710085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>try make</td>\n",
       "      <td>-4.653371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>minute</td>\n",
       "      <td>-4.535722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>try</td>\n",
       "      <td>-4.520451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>religion</td>\n",
       "      <td>3.746690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>pulp</td>\n",
       "      <td>3.746862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>delightful</td>\n",
       "      <td>3.852925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>frank</td>\n",
       "      <td>3.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>life</td>\n",
       "      <td>3.893409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>wonderfully</td>\n",
       "      <td>3.911728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>solid</td>\n",
       "      <td>3.994578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>4.009029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>normal</td>\n",
       "      <td>4.077547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>effective</td>\n",
       "      <td>4.111080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>portray</td>\n",
       "      <td>4.125987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>truman</td>\n",
       "      <td>4.147696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>excellent</td>\n",
       "      <td>4.380454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>different</td>\n",
       "      <td>4.697547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>perfect</td>\n",
       "      <td>4.709628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>matrix</td>\n",
       "      <td>4.752213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>especially</td>\n",
       "      <td>4.776480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>5.110393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>quite</td>\n",
       "      <td>5.304986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>memorable</td>\n",
       "      <td>5.404215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>5.603945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>today</td>\n",
       "      <td>5.771444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>overall</td>\n",
       "      <td>5.809448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>daylight</td>\n",
       "      <td>5.992663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>performance</td>\n",
       "      <td>6.239876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>perfectly</td>\n",
       "      <td>6.253804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>hilarious</td>\n",
       "      <td>6.297132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>terrific</td>\n",
       "      <td>6.435204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>definitely</td>\n",
       "      <td>6.494277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>great</td>\n",
       "      <td>9.794397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  coefficient\n",
       "81              bad   -12.740770\n",
       "1449          waste    -8.606913\n",
       "1307        suppose    -8.403378\n",
       "70          attempt    -8.137514\n",
       "145          boring    -7.687704\n",
       "1007           plot    -7.204338\n",
       "1400  unfortunately    -6.920251\n",
       "1020           poor    -6.274391\n",
       "849            mess    -5.804681\n",
       "1120     ridiculous    -5.763368\n",
       "356            dull    -5.746076\n",
       "832           maybe    -5.607562\n",
       "411            fail    -5.531238\n",
       "78            awful    -5.514160\n",
       "210           cheap    -5.281592\n",
       "1022         poorly    -5.255674\n",
       "536           guess    -5.236280\n",
       "1340       terrible    -5.129538\n",
       "455            flat    -5.107622\n",
       "1028      potential    -5.026281\n",
       "1293         stupid    -4.976049\n",
       "1175         script    -4.918184\n",
       "380    embarrassing    -4.835148\n",
       "735            lame    -4.776236\n",
       "1334        tedious    -4.752035\n",
       "823        material    -4.718103\n",
       "605            idea    -4.710085\n",
       "1386       try make    -4.653371\n",
       "862          minute    -4.535722\n",
       "1383            try    -4.520451\n",
       "...             ...          ...\n",
       "1108       religion     3.746690\n",
       "1065           pulp     3.746862\n",
       "309      delightful     3.852925\n",
       "468           frank     3.854641\n",
       "762            life     3.893409\n",
       "1480    wonderfully     3.911728\n",
       "1245          solid     3.994578\n",
       "959     outstanding     4.009029\n",
       "929          normal     4.077547\n",
       "369       effective     4.111080\n",
       "1026        portray     4.125987\n",
       "1381         truman     4.147696\n",
       "395       excellent     4.380454\n",
       "325       different     4.697547\n",
       "981         perfect     4.709628\n",
       "824          matrix     4.752213\n",
       "390      especially     4.776480\n",
       "110       beautiful     5.110393\n",
       "1073          quite     5.304986\n",
       "843       memorable     5.404215\n",
       "1479      wonderful     5.603945\n",
       "1362          today     5.771444\n",
       "961         overall     5.809448\n",
       "299        daylight     5.992663\n",
       "986     performance     6.239876\n",
       "985       perfectly     6.253804\n",
       "572       hilarious     6.297132\n",
       "1342       terrific     6.435204\n",
       "305      definitely     6.494277\n",
       "523           great     9.794397\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names() # get the names of the features\n",
    "top_scores = selector.scores_.argsort()[-1500:] # get the indices of the selection\n",
    "best_indicator_terms = [features[i] for i in sorted(top_scores)] # sort feature names\n",
    "\n",
    "top_indicator_scores = pd.DataFrame(data={'feature': best_indicator_terms, 'coefficient': clf.coef_[0]})\n",
    "top_indicator_scores.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Checklist: how to classify my data\n",
    "\n",
    "1. label at ***least 2000*** tweets in your data set as `positive`, `negative`, or `neutral`\n",
    "2. preprocess the text of *all* tweets in your data (labeled and unlabeled)\n",
    "3. read in the labeled tweets and their labels\n",
    "4. transform the labels into numbers\n",
    "5. use `TfidfVectorizer` to extract the features and transform them into feature vectors\n",
    "6. select the top $N$ features (where $N$ is smaller than the number of labeled tweets)\n",
    "7. create a classifier\n",
    "8. use 5-fold CV to find the best regularization parameter, top $N$ feature selection, and maybe feature generation and preprocessing steps\n",
    "\n",
    "Once you are satisfied with the results:\n",
    "9. read in the rest of the (unlabeled) tweets\n",
    "10. use the `TfidfVectorizer` from 5. to transform the new data into vectors\n",
    "11. use the `SelectKBest` selector from 6. to get the top $N$ features\n",
    "12. use the classifier from 7. to predict the labels for the new data\n",
    "13. save the predicted labels or probabilities to your database or an Excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
